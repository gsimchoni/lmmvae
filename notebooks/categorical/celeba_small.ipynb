{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from lmmvae.dim_reduction_images import run_dim_reduction_images\n",
    "from lmmvae.simulation import Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_file</th>\n",
       "      <th>lefteye_x</th>\n",
       "      <th>lefteye_y</th>\n",
       "      <th>righteye_x</th>\n",
       "      <th>righteye_y</th>\n",
       "      <th>nose_x</th>\n",
       "      <th>nose_y</th>\n",
       "      <th>leftmouth_x</th>\n",
       "      <th>leftmouth_y</th>\n",
       "      <th>rightmouth_x</th>\n",
       "      <th>rightmouth_y</th>\n",
       "      <th>celeb_orig</th>\n",
       "      <th>celeb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001.png</td>\n",
       "      <td>69</td>\n",
       "      <td>109</td>\n",
       "      <td>106</td>\n",
       "      <td>113</td>\n",
       "      <td>77</td>\n",
       "      <td>142</td>\n",
       "      <td>73</td>\n",
       "      <td>152</td>\n",
       "      <td>108</td>\n",
       "      <td>154</td>\n",
       "      <td>2880</td>\n",
       "      <td>1540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002.png</td>\n",
       "      <td>69</td>\n",
       "      <td>110</td>\n",
       "      <td>107</td>\n",
       "      <td>112</td>\n",
       "      <td>81</td>\n",
       "      <td>135</td>\n",
       "      <td>70</td>\n",
       "      <td>151</td>\n",
       "      <td>108</td>\n",
       "      <td>153</td>\n",
       "      <td>2937</td>\n",
       "      <td>1573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000003.png</td>\n",
       "      <td>76</td>\n",
       "      <td>112</td>\n",
       "      <td>104</td>\n",
       "      <td>106</td>\n",
       "      <td>108</td>\n",
       "      <td>128</td>\n",
       "      <td>74</td>\n",
       "      <td>156</td>\n",
       "      <td>98</td>\n",
       "      <td>158</td>\n",
       "      <td>8692</td>\n",
       "      <td>4689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000004.png</td>\n",
       "      <td>72</td>\n",
       "      <td>113</td>\n",
       "      <td>108</td>\n",
       "      <td>108</td>\n",
       "      <td>101</td>\n",
       "      <td>138</td>\n",
       "      <td>71</td>\n",
       "      <td>155</td>\n",
       "      <td>101</td>\n",
       "      <td>151</td>\n",
       "      <td>5805</td>\n",
       "      <td>3128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000005.png</td>\n",
       "      <td>66</td>\n",
       "      <td>114</td>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>86</td>\n",
       "      <td>119</td>\n",
       "      <td>71</td>\n",
       "      <td>147</td>\n",
       "      <td>104</td>\n",
       "      <td>150</td>\n",
       "      <td>9295</td>\n",
       "      <td>5005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     img_file  lefteye_x  lefteye_y  righteye_x  righteye_y  nose_x  nose_y  \\\n",
       "0  000001.png         69        109         106         113      77     142   \n",
       "1  000002.png         69        110         107         112      81     135   \n",
       "2  000003.png         76        112         104         106     108     128   \n",
       "3  000004.png         72        113         108         108     101     138   \n",
       "4  000005.png         66        114         112         112      86     119   \n",
       "\n",
       "   leftmouth_x  leftmouth_y  rightmouth_x  rightmouth_y  celeb_orig  celeb  \n",
       "0           73          152           108           154        2880   1540  \n",
       "1           70          151           108           153        2937   1573  \n",
       "2           74          156            98           158        8692   4689  \n",
       "3           71          155           101           151        5805   3128  \n",
       "4           71          147           104           150        9295   5005  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_df = pd.read_csv('../../data/celeba_small.csv')\n",
    "\n",
    "images_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def read_image(img_file, height=72, width=60):\n",
    "    img = Image.open(img_file)\n",
    "    img = img.resize((width, height))\n",
    "    img = np.asarray(img, dtype=np.float32) / 255.\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 72, 60, 3)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "img_path = '../../data/img_align_celeba_png/'\n",
    "for img_file in images_df['img_file']:\n",
    "    # images.append(plt.imread(img_path + img_file))\n",
    "    images.append(read_image(img_path +  img_file))\n",
    "\n",
    "X = np.array(images)\n",
    "\n",
    "RE_cols = ['celeb']\n",
    "Z = images_df[RE_cols].values\n",
    "\n",
    "print(X.shape) # (10000, 72, 60, 3)\n",
    "print(Z.shape) # (10000, 1)\n",
    "del images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "518400176"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pympler import asizeof\n",
    "asizeof.asizeof(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of sources: 5429\n"
     ]
    }
   ],
   "source": [
    "n_cats_celebs = len(images_df['celeb'].unique())\n",
    "print(f'no. of sources: {n_cats_celebs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params for LMMVAE and other methods, some unnecessary for current use-case therefore are none\n",
    "img_height, img_width, channels = X.shape[1:]\n",
    "mode = 'categorical'\n",
    "n_sig2bs = 1\n",
    "n_sig2bs_spatial = 0\n",
    "n_neurons = [32, 16]\n",
    "dropout = None\n",
    "activation = 'relu'\n",
    "RE_cols_prefix = 'z'\n",
    "thresh = None\n",
    "epochs = 200\n",
    "qs = [n_cats_celebs]\n",
    "q_spatial = None\n",
    "batch_size = 1000\n",
    "patience = None\n",
    "kernel_root = None\n",
    "U = None\n",
    "B_list = None\n",
    "est_cors = []\n",
    "n_neurons_re = n_neurons\n",
    "pred_unknown_clusters = False\n",
    "max_spatial_locs = 100\n",
    "time2measure_dict = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(columns=['d', 'beta', 're_prior', 'experiment', 'exp_type', 'mse_X', 'sigma_b0_est', 'n_epoch', 'time',\n",
    "    'total_loss_tr', 'recon_loss_tr', 'kl_loss_tr', 're_kl_loss_tr', 'total_loss_te', 'recon_loss_te', 'kl_loss_te', 're_kl_loss_te'\n",
    "    ])\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=40)\n",
    "counter = Count().gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_reg_types(X_train, X_test, Z_train, Z_test, counter, d, beta, re_prior, i, verbose):\n",
    "    mse_lmmvae, sigmas, _, n_epochs_lmmvae, time_lmmvae, losses_lmmvae = run_dim_reduction_images(X_train, X_test, Z_train, Z_test,\n",
    "            img_height, img_width, channels, d, 'lmmvae',\n",
    "            thresh, epochs, qs, q_spatial, n_sig2bs, n_sig2bs_spatial, est_cors, batch_size, patience, n_neurons, n_neurons_re, dropout,\n",
    "            activation, mode, beta, re_prior, kernel_root, pred_unknown_clusters, max_spatial_locs, time2measure_dict, verbose, U, B_list)\n",
    "    print('   finished lmmvae, mse: %.3f' % mse_lmmvae)\n",
    "    mse_ig, _, _, n_epochs_ig, time_ig, losses_ig = run_dim_reduction_images(X_train, X_test, Z_train, Z_test,\n",
    "            img_height, img_width, channels, d, 'pca-ignore',\n",
    "            thresh, epochs, qs, q_spatial, n_sig2bs, n_sig2bs_spatial, est_cors, batch_size, patience, n_neurons, n_neurons_re, dropout,\n",
    "            activation, mode, beta, re_prior, kernel_root, pred_unknown_clusters, max_spatial_locs, time2measure_dict, verbose, U, B_list)\n",
    "    print('   finished pca-ignore, mse: %.3f' % mse_ig)\n",
    "    mse_ohe, _, _, n_epochs_ohe, time_ohe, losses_ohe = run_dim_reduction_images(X_train, X_test, Z_train, Z_test,\n",
    "            img_height, img_width, channels, d, 'pca-ohe',\n",
    "            thresh, epochs, qs, q_spatial, n_sig2bs, n_sig2bs_spatial, est_cors, batch_size, patience, n_neurons, n_neurons_re, dropout,\n",
    "            activation, mode, beta, re_prior, kernel_root, pred_unknown_clusters, max_spatial_locs, time2measure_dict, verbose, U, B_list)\n",
    "    print('   finished pca-ohe, mse: %.3f' % mse_ohe)\n",
    "    mse_vaeig, _, _, n_epochs_vaeig, time_vaeig, losses_vaeig = run_dim_reduction_images(X_train, X_test, Z_train, Z_test,\n",
    "            img_height, img_width, channels, d, 'vae-ignore',\n",
    "            thresh, epochs, qs, q_spatial, n_sig2bs, n_sig2bs_spatial, est_cors, batch_size, patience, n_neurons, n_neurons_re, dropout,\n",
    "            activation, mode, beta, re_prior, kernel_root, pred_unknown_clusters, max_spatial_locs, time2measure_dict, verbose, U, B_list)\n",
    "    print('   finished vae-ignore, mse: %.3f' % mse_vaeig)\n",
    "    mse_vaeem, _, _, n_epochs_vaeem, time_vaeem, losses_vaeem = run_dim_reduction_images(X_train, X_test, Z_train, Z_test,\n",
    "            img_height, img_width, channels, d, 'vae-embed',\n",
    "            thresh, epochs, qs, q_spatial, n_sig2bs, n_sig2bs_spatial, est_cors, batch_size, patience, n_neurons, n_neurons_re, dropout,\n",
    "            activation, mode, beta, re_prior, kernel_root, pred_unknown_clusters, max_spatial_locs, time2measure_dict, verbose, U, B_list)\n",
    "    print('   finished vae-embed, mse: %.3f' % mse_vaeem)\n",
    "    res.loc[next(counter)] = [d, beta, re_prior, i, 'lmmvae', mse_lmmvae, sigmas[1][0], n_epochs_lmmvae, time_lmmvae] + losses_lmmvae\n",
    "    res.loc[next(counter)] = [d, beta, re_prior, i, 'pca-ignore', mse_ig, np.nan, n_epochs_ig, time_ig] + losses_ig\n",
    "    res.loc[next(counter)] = [d, beta, re_prior, i, 'pca-ohe', mse_ohe, np.nan, n_epochs_ohe, time_ohe] + losses_ohe\n",
    "    res.loc[next(counter)] = [d, beta, re_prior, i, 'vae-ignore', mse_vaeig, np.nan, n_epochs_vaeig, time_vaeig] + losses_vaeig\n",
    "    res.loc[next(counter)] = [d, beta, re_prior, i, 'vae-embed', mse_vaeem, np.nan, n_epochs_vaeem, time_vaeem] + losses_vaeem\n",
    "    res.to_csv('res_celeba.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = [0.01]\n",
    "ds = [100, 200, 500]\n",
    "re_priors = [0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.01, d: 100, re_prior: 0.001:\n",
      "  iteration 0\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 9s 782ms/step - loss: 72054.6094 - recon_loss: 13563.7217 - kl_loss: 1071.5677 - re_kl_loss: 5848018.0000 - val_loss: 50861.8672 - val_recon_loss: 9268.0215 - val_kl_loss: 125.4413 - val_re_kl_loss: 4159259.2500\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 5s 657ms/step - loss: 28714.7207 - recon_loss: 5602.9624 - kl_loss: 446.6894 - re_kl_loss: 2310729.0000 - val_loss: 8934.6416 - val_recon_loss: 2535.3352 - val_kl_loss: 402.2456 - val_re_kl_loss: 639528.3750\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 5s 657ms/step - loss: 4948.3706 - recon_loss: 1703.5702 - kl_loss: 244.5865 - re_kl_loss: 324235.4688 - val_loss: 2384.1868 - val_recon_loss: 1132.9336 - val_kl_loss: 187.6876 - val_re_kl_loss: 124937.6328\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 5s 656ms/step - loss: 1970.6761 - recon_loss: 1052.9246 - kl_loss: 175.1131 - re_kl_loss: 91600.0703 - val_loss: 1617.4336 - val_recon_loss: 986.0141 - val_kl_loss: 135.4431 - val_re_kl_loss: 63006.5039\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 5s 657ms/step - loss: 1460.0464 - recon_loss: 928.7694 - kl_loss: 128.2318 - re_kl_loss: 52999.4570 - val_loss: 1341.8433 - val_recon_loss: 892.0357 - val_kl_loss: 125.0584 - val_re_kl_loss: 44855.6953\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 5s 658ms/step - loss: 1269.2506 - recon_loss: 873.6573 - kl_loss: 114.3686 - re_kl_loss: 39444.9727 - val_loss: 1223.7703 - val_recon_loss: 860.2307 - val_kl_loss: 122.5252 - val_re_kl_loss: 36231.4375\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 5s 656ms/step - loss: 1170.4055 - recon_loss: 840.9216 - kl_loss: 114.8106 - re_kl_loss: 32833.5898 - val_loss: 1148.0148 - val_recon_loss: 832.3548 - val_kl_loss: 121.3213 - val_re_kl_loss: 31444.6719\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 5s 657ms/step - loss: 1123.2140 - recon_loss: 828.6670 - kl_loss: 119.6722 - re_kl_loss: 29335.0273 - val_loss: 1107.3519 - val_recon_loss: 817.2396 - val_kl_loss: 130.6109 - val_re_kl_loss: 28880.6191\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 5s 653ms/step - loss: 1084.7532 - recon_loss: 811.8768 - kl_loss: 130.4228 - re_kl_loss: 27157.2246 - val_loss: 1079.8101 - val_recon_loss: 807.5649 - val_kl_loss: 141.4240 - val_re_kl_loss: 27083.0996\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m  iteration \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m i)\n\u001b[0;32m     15\u001b[0m X_train, X_test, Z_train, Z_test \u001b[39m=\u001b[39m X[train_index]\u001b[39m.\u001b[39mcopy(), X[test_index]\u001b[39m.\u001b[39mcopy(), Z[train_index]\u001b[39m.\u001b[39mcopy(), Z[test_index]\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m---> 16\u001b[0m iterate_reg_types(X_train, X_test, Z_train, Z_test, counter, d, beta, re_prior, i, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m, in \u001b[0;36miterate_reg_types\u001b[1;34m(X_train, X_test, Z_train, Z_test, counter, d, beta, re_prior, i, verbose)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39miterate_reg_types\u001b[39m(X_train, X_test, Z_train, Z_test, counter, d, beta, re_prior, i, verbose):\n\u001b[1;32m----> 2\u001b[0m     mse_lmmvae, sigmas, _, n_epochs_lmmvae, time_lmmvae, losses_lmmvae \u001b[39m=\u001b[39m run_dim_reduction_images(X_train, X_test, Z_train, Z_test,\n\u001b[0;32m      3\u001b[0m             img_height, img_width, channels, d, \u001b[39m'\u001b[39;49m\u001b[39mlmmvae\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m      4\u001b[0m             thresh, epochs, qs, q_spatial, n_sig2bs, n_sig2bs_spatial, est_cors, batch_size, patience, n_neurons, n_neurons_re, dropout,\n\u001b[0;32m      5\u001b[0m             activation, mode, beta, re_prior, kernel_root, pred_unknown_clusters, max_spatial_locs, time2measure_dict, verbose, U, B_list)\n\u001b[0;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m   finished lmmvae, mse: \u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m mse_lmmvae)\n\u001b[0;32m      7\u001b[0m     mse_ig, _, _, n_epochs_ig, time_ig, losses_ig \u001b[39m=\u001b[39m run_dim_reduction_images(X_train, X_test, Z_train, Z_test,\n\u001b[0;32m      8\u001b[0m             img_height, img_width, channels, d, \u001b[39m'\u001b[39m\u001b[39mpca-ignore\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      9\u001b[0m             thresh, epochs, qs, q_spatial, n_sig2bs, n_sig2bs_spatial, est_cors, batch_size, patience, n_neurons, n_neurons_re, dropout,\n\u001b[0;32m     10\u001b[0m             activation, mode, beta, re_prior, kernel_root, pred_unknown_clusters, max_spatial_locs, time2measure_dict, verbose, U, B_list)\n",
      "File \u001b[1;32mc:\\Users\\gsimchoni\\lmmvae\\lmmvae\\dim_reduction_images.py:215\u001b[0m, in \u001b[0;36mrun_dim_reduction_images\u001b[1;34m(X_train, X_test, Z_train, Z_test, img_height, img_width, channels, d, dr_type, thresh, epochs, qs, q_spatial, n_sig2bs, n_sig2bs_spatial, est_cors, batch_size, patience, n_neurons, n_neurons_re, dropout, activation, mode, beta, re_prior, kernel, pred_unknown_clusters, max_spatial_locs, time2measure_dict, verbose, U, B_list, train_generator, valid_generator, test_generator, train_RE_inputs)\u001b[0m\n\u001b[0;32m    210\u001b[0m     X_reconstructed_te, sigmas, n_epochs, losses \u001b[39m=\u001b[39m run_vae_images(\n\u001b[0;32m    211\u001b[0m         X_train, X_test, Z_train, Z_test, img_height, img_width, channels, qs, d, n_sig2bs_spatial, batch_size,\n\u001b[0;32m    212\u001b[0m         epochs, patience, n_neurons, dropout, activation, mode, n_sig2bs, beta, pred_unknown_clusters, verbose, embed_RE\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, cnn\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    213\u001b[0m         is_generator\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, train_generator\u001b[39m=\u001b[39mtrain_generator, valid_generator\u001b[39m=\u001b[39mvalid_generator, test_generator\u001b[39m=\u001b[39mtest_generator)\n\u001b[0;32m    214\u001b[0m \u001b[39melif\u001b[39;00m dr_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mlmmvae\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m     X_reconstructed_te, sigmas, n_epochs, losses \u001b[39m=\u001b[39m run_lmmvae_images(\n\u001b[0;32m    216\u001b[0m         X_train, X_test, Z_train, Z_test, img_height, img_width, channels, qs, q_spatial, d, n_sig2bs, n_sig2bs_spatial, re_prior, batch_size,\n\u001b[0;32m    217\u001b[0m         epochs, patience, n_neurons, n_neurons_re, dropout, activation, mode, beta, kernel, pred_unknown_clusters,\n\u001b[0;32m    218\u001b[0m         max_spatial_locs, verbose, U, B_list)\n\u001b[0;32m    219\u001b[0m \u001b[39melif\u001b[39;00m dr_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mlmmvae-gen\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    220\u001b[0m     X_reconstructed_te, sigmas, n_epochs, losses \u001b[39m=\u001b[39m run_lmmvae_images(\n\u001b[0;32m    221\u001b[0m         X_train, X_test, Z_train, Z_test, img_height, img_width, channels, qs, q_spatial, d, n_sig2bs, n_sig2bs_spatial, re_prior, batch_size,\n\u001b[0;32m    222\u001b[0m         epochs, patience, n_neurons, n_neurons_re, dropout, activation, mode, beta, kernel, pred_unknown_clusters,\n\u001b[0;32m    223\u001b[0m         max_spatial_locs, verbose, U, B_list,\n\u001b[0;32m    224\u001b[0m         is_generator\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, train_generator\u001b[39m=\u001b[39mtrain_generator, valid_generator\u001b[39m=\u001b[39mvalid_generator,\n\u001b[0;32m    225\u001b[0m         test_generator\u001b[39m=\u001b[39mtest_generator, train_RE_inputs \u001b[39m=\u001b[39m train_RE_inputs)\n",
      "File \u001b[1;32mc:\\Users\\gsimchoni\\lmmvae\\lmmvae\\dim_reduction_images.py:126\u001b[0m, in \u001b[0;36mrun_lmmvae_images\u001b[1;34m(X_train, X_test, Z_train, Z_test, img_height, img_width, channels, qs, q_spatial, d, n_sig2bs, n_sig2bs_spatial, re_prior, batch_size, epochs, patience, n_neurons, n_neurons_re, dropout, activation, mode, beta, kernel_root, pred_unknown_clusters, max_spatial_locs, verbose, U, B_list, cnn, is_generator, train_generator, valid_generator, test_generator, train_RE_inputs)\u001b[0m\n\u001b[0;32m    124\u001b[0m     losses_te \u001b[39m=\u001b[39m lmmvae\u001b[39m.\u001b[39mevaluate_gen(test_generator)\n\u001b[0;32m    125\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 126\u001b[0m     X_transformed_tr, B_hat_list, sig2bs_hat_list \u001b[39m=\u001b[39m lmmvae\u001b[39m.\u001b[39;49mfit_transform(X_train, Z_train, U, B_list)\n\u001b[0;32m    127\u001b[0m     X_transformed_te, _, _ \u001b[39m=\u001b[39m lmmvae\u001b[39m.\u001b[39mtransform(X_test, Z_test, U, B_list)\n\u001b[0;32m    128\u001b[0m     X_reconstructed_te \u001b[39m=\u001b[39m lmmvae\u001b[39m.\u001b[39mreconstruct(X_transformed_te, Z_test, B_hat_list)\n",
      "File \u001b[1;32mc:\\Users\\gsimchoni\\lmmvae\\lmmvae\\vae_images.py:628\u001b[0m, in \u001b[0;36mLMMVAEIMG.fit_transform\u001b[1;34m(self, X, Z, U, B_list, reconstruct_B)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_transform\u001b[39m(\u001b[39mself\u001b[39m, X, Z, U, B_list, reconstruct_B\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m--> 628\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, Z)\n\u001b[0;32m    629\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transform(X, Z, U, B_list, reconstruct_B)\n",
      "File \u001b[1;32mc:\\Users\\gsimchoni\\lmmvae\\lmmvae\\vae_images.py:546\u001b[0m, in \u001b[0;36mLMMVAEIMG._fit\u001b[1;34m(self, X, Z)\u001b[0m\n\u001b[0;32m    542\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvariational_ae\u001b[39m.\u001b[39mfit([X_input_train] \u001b[39m+\u001b[39m Z_inputs_train, X_train, epochs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs,\n\u001b[0;32m    543\u001b[0m         callbacks\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks, batch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size, validation_data\u001b[39m=\u001b[39m([X_input_valid] \u001b[39m+\u001b[39m Z_inputs_valid, X_valid),\n\u001b[0;32m    544\u001b[0m         verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m    545\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 546\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvariational_ae\u001b[39m.\u001b[39;49mfit(X_inputs \u001b[39m+\u001b[39;49m Z_inputs, X, epochs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepochs,\n\u001b[0;32m    547\u001b[0m         callbacks\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcallbacks, batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,\n\u001b[0;32m    548\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose)\n\u001b[0;32m    549\u001b[0m gc\u001b[39m.\u001b[39mcollect()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py:1414\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1412\u001b[0m logs \u001b[39m=\u001b[39m tmp_logs  \u001b[39m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[0;32m   1413\u001b[0m end_step \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m data_handler\u001b[39m.\u001b[39mstep_increment\n\u001b[1;32m-> 1414\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_train_batch_end(end_step, logs)\n\u001b[0;32m   1415\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n\u001b[0;32m   1416\u001b[0m   \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\callbacks.py:438\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[39m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \n\u001b[0;32m    433\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[39m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[39m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 438\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook(ModeKeys\u001b[39m.\u001b[39;49mTRAIN, \u001b[39m'\u001b[39;49m\u001b[39mend\u001b[39;49m\u001b[39m'\u001b[39;49m, batch, logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\callbacks.py:297\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    295\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    296\u001b[0m \u001b[39melif\u001b[39;00m hook \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mend\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 297\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_end_hook(mode, batch, logs)\n\u001b[0;32m    298\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    300\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mUnrecognized hook: \u001b[39m\u001b[39m{\u001b[39;00mhook\u001b[39m}\u001b[39;00m\u001b[39m. Expected values are [\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\callbacks.py:318\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m   batch_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_start_time\n\u001b[0;32m    316\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times\u001b[39m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 318\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook_helper(hook_name, batch, logs)\n\u001b[0;32m    320\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    321\u001b[0m   end_hook_name \u001b[39m=\u001b[39m hook_name\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\callbacks.py:356\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[0;32m    355\u001b[0m   hook \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 356\u001b[0m   hook(batch, logs)\n\u001b[0;32m    358\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timing:\n\u001b[0;32m    359\u001b[0m   \u001b[39mif\u001b[39;00m hook_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\callbacks.py:1034\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1033\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_batch_end\u001b[39m(\u001b[39mself\u001b[39m, batch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m-> 1034\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_update_progbar(batch, logs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\callbacks.py:1106\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1102\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m add_seen\n\u001b[0;32m   1104\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1105\u001b[0m   \u001b[39m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m-> 1106\u001b[0m   logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39;49msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   1107\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogbar\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen, \u001b[39mlist\u001b[39m(logs\u001b[39m.\u001b[39mitems()), finalize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\tf_utils.py:607\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    604\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n\u001b[0;32m    605\u001b[0m   \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mitem() \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mndim(t) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m t\n\u001b[1;32m--> 607\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mmap_structure(_to_single_numpy_or_python_type, tensors)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\nest.py:916\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    912\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    913\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    915\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 916\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    917\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\nest.py:916\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    912\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    913\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    915\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 916\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    917\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\tf_utils.py:601\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    599\u001b[0m   \u001b[39m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m    600\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, tf\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m--> 601\u001b[0m     t \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39;49mnumpy()\n\u001b[0;32m    602\u001b[0m   \u001b[39m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[39;00m\n\u001b[0;32m    603\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(t, (np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mgeneric)):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py:1159\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1136\u001b[0m \u001b[39m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \n\u001b[0;32m   1138\u001b[0m \u001b[39mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1156\u001b[0m \u001b[39m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1157\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[39m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1159\u001b[0m maybe_arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1160\u001b[0m \u001b[39mreturn\u001b[39;00m maybe_arr\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(maybe_arr, np\u001b[39m.\u001b[39mndarray) \u001b[39melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py:1125\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_numpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1124\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1125\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy_internal()\n\u001b[0;32m   1126\u001b[0m   \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m     \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for beta in betas:\n",
    "  for d in ds:\n",
    "    for re_prior in re_priors:\n",
    "      print(f'beta: {beta}, d: {d}, re_prior: {re_prior}:')\n",
    "      if pred_unknown_clusters:\n",
    "        for i, (train_samp_subj, test_samp_subj) in enumerate(kf.split(range(n_cats_celebs))):\n",
    "          print('  iteration %d' % i)\n",
    "          train_index = images_df[images_df['celeb'].isin(train_samp_subj)].index.values\n",
    "          test_index = images_df[images_df['celeb'].isin(test_samp_subj)].index.values\n",
    "          X_train, X_test, Z_train, Z_test = X[train_index].copy(), X[test_index].copy(), Z[train_index].copy(), Z[test_index].copy()\n",
    "          iterate_reg_types(X_train, X_test, Z_train, Z_test, counter, d, beta, re_prior, i, verbose=True)\n",
    "      else:\n",
    "        for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "          print('  iteration %d' % i)\n",
    "          X_train, X_test, Z_train, Z_test = X[train_index].copy(), X[test_index].copy(), Z[train_index].copy(), Z[test_index].copy()\n",
    "          iterate_reg_types(X_train, X_test, Z_train, Z_test, counter, d, beta, re_prior, i, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "58232888a71497b14c2600f41783e14bf8a4ed364d880e61f45b8308dbbef17d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
